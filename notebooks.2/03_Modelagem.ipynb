{
 "cells": [],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# ============================================================================
# IMPORTS NECESSÁRIOS
# ============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib

# Configurações de visualização
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("viridis")

# ============================================================================
# PASSO 1: CARREGAR DADOS LIMPOS
# ============================================================================

print("="*60)
print("ETAPA 3: MODELO BASELINE - REGRESSÃO LINEAR")
print("="*60)

# TODO: 1. Ajuste o caminho para seu arquivo de dados limpos (students_clean.csv)
# Este arquivo deve conter as 63 colunas após o pré-processamento.
try:
    # CORREÇÃO AQUI: Apenas um '../' se o script está em 'notebooks/'
    df = pd.read_csv('../data/students_clean.csv')
    print(f"\n✓ Dados carregados: {df.shape[0]} linhas, {df.shape[1]} colunas")
except FileNotFoundError:
    # O bloco 'except' é o que dispara o erro que você viu no terminal.
    print("\nERRO: O arquivo 'students_clean.csv' não foi encontrado. Verifique o caminho.")
    exit()

# ============================================================================
# PASSO 2: SEPARAR FEATURES (X) e TARGET (y)
# ============================================================================

# TODO: 2. Confirme o nome da coluna target
TARGET_COLUMN = 'final_grade'

# Colunas a remover que não devem ser features (se houver, ex: ID)
COLUNAS_REMOVER = ['student_id']

# Separar X (features) e y (target)
X = df.drop(columns=[TARGET_COLUMN] + COLUNAS_REMOVER, errors='ignore')
y = df[TARGET_COLUMN]

print(f"✓ Features (X): {X.shape[1]} colunas. Target (y): {y.shape[0]} valores.")

# ============================================================================
# PASSO 3: DIVISÃO DOS DADOS (60% TREINO / 20% VALIDAÇÃO / 20% TESTE)
# ============================================================================

RANDOM_STATE = 42

# 1. Separar 20% para teste (NÃO SERÁ USADO AGORA)
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE
)

# 2. Dos 80% restantes (X_temp), separar 25% para validação (25% de 80% = 20% do total)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=RANDOM_STATE
)

print("\n" + "="*60)
print("DIVISÃO DOS DADOS: 60% TREINO / 20% VALIDAÇÃO / 20% TESTE")
print("="*60)
total_samples = len(X)
print(f"├─ Treino:     {len(X_train)} amostras")
print(f"├─ Validação:  {len(X_val)} amostras")
print(f"└─ Teste:      {len(X_test)} amostras (Guardado para Etapa 5)")
print(f"\nVerificado: Não há 'data leakage' na divisão. ")

# ============================================================================
# PASSO 4: CRIAR E TREINAR MODELO BASELINE (REGRESSÃO LINEAR)
# ============================================================================

print("\n" + "="*60)
print("TREINAMENTO DO MODELO")
print("="*60)

modelo = LinearRegression()

# Treinar modelo APENAS no conjunto de TREINO
modelo.fit(X_train, y_train)
print("✓ Modelo de Regressão Linear treinado!")

# ============================================================================
# PASSO 5: ANÁLISE DE COEFICIENTES (IMPORTÂNCIA DAS FEATURES)
# ============================================================================

# Criar DataFrame com features e coeficientes (pesos)
coeficientes = pd.DataFrame({
    'Feature': X_train.columns,
    'Coeficiente': modelo.coef_
}).sort_values('Coeficiente', key=abs, ascending=False).head(5) # Top 5

print("\n" + "-"*60)
print("TOP 5 FEATURES MAIS IMPORTANTES (COEFICIENTES)")
print("-"*60)
print(coeficientes.to_string(index=False))
print(f"\nIntercept (nota base): {modelo.intercept_:.4f}")

# TODO: 3. Interprete o coeficiente mais importante no seu relatório
# Exemplo de interpretação:
# print(f"\nInsight chave: O coeficiente mais alto ({coeficientes.iloc[0]['Feature']}) indica um impacto direto de {coeficientes.iloc[0]['Coeficiente']:+.3f} pontos na nota final.")

# ============================================================================
# PASSO 6: FAZER PREVISÕES E AVALIAR MÉTRICAS
# ============================================================================

# Previsões nos conjuntos de treino e validação
y_train_pred = modelo.predict(X_train)
y_val_pred = modelo.predict(X_val)

def calcular_metricas(y_true, y_pred, conjunto):
    """Calcula e retorna R², RMSE, MAE e MSE."""
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    return {'Conjunto': conjunto, 'R2': r2, 'RMSE': rmse, 'MAE': mae, 'MSE': mse}

# Calcular métricas
metricas_train = calcular_metricas(y_train, y_train_pred, 'Treino')
metricas_val = calcular_metricas(y_val, y_val_pred, 'Validação')

metricas_df = pd.DataFrame([metricas_train, metricas_val])

print("\n" + "="*60)
print("MÉTRICAS DE PERFORMANCE DO MODELO BASELINE")
print("="*60)
print(metricas_df.set_index('Conjunto').to_markdown())

# Análise de Overfitting (Diferença no R²)
r2_diff = abs(metricas_train['R2'] - metricas_val['R2'])
if r2_diff < 0.10:
    status_overfitting = "✅ Sem overfitting (Generaliza bem)"
else:
    status_overfitting = "❌ Risco de Overfitting (Revisar na Etapa 4)"

print(f"\nDiferença no R² entre Treino e Validação: {r2_diff:.4f} → {status_overfitting}")

# ============================================================================
# PASSO 7: GERAÇÃO DE VISUALIZAÇÕES
# ============================================================================

# 1. Gráfico: Predições vs Valores Reais
plt.figure(figsize=(10, 6))
plt.scatter(y_val, y_val_pred, alpha=0.6)
# Linha diagonal de predição perfeita
max_val = max(y_val.max(), y_val_pred.max())
min_val = min(y_val.min(), y_val_pred.min())
plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
plt.title(f'Validação: Predições vs Valores Reais (R²: {metricas_val["R2"]:.2f})')
plt.xlabel(f'Nota Real ({TARGET_COLUMN})')
plt.ylabel('Nota Prevista')
plt.grid(True)
plt.show()
print("✓ Gráfico 1 gerado: Predições vs Valores Reais. ")

# 2. Gráfico: Distribuição de Resíduos
residuos = y_val - y_val_pred
plt.figure(figsize=(10, 6))
sns.histplot(residuos, kde=True, bins=30)
plt.axvline(x=0, color='r', linestyle='--')
plt.title(f'Distribuição dos Resíduos (Média: {residuos.mean():.4f})')
plt.xlabel('Resíduo (Valor Real - Valor Previsto)')
plt.ylabel('Frequência')
plt.grid(True)
plt.show()
print("✓ Gráfico 2 gerado: Distribuição dos Resíduos. ")

# ============================================================================
# PASSO 8: SALVAR MODELO
# ============================================================================

# TODO: 4. Salvar o modelo (necessário para a Etapa 4)
MODEL_PATH = '../models/baseline.pkl'
joblib.dump(modelo, MODEL_PATH)
print(f"\n✓ Modelo 'baseline.pkl' salvo em: {MODEL_PATH}")

print("\n" + "="*60)
print("ETAPA 3 CONCLUÍDA. DOCUMENTAÇÃO PRONTA.")
print("="*60)
